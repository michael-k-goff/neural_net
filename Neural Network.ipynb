{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "This project implements a simple neural network. The purpose is to demonstrate the basic structure. The network is tested on the [MNIST handwritten digit data set](http://yann.lecun.com/exdb/mnist/). This exercise is inspired by Andrew Ng's Deep Learning series on Coursera. Dr. Ng suggests that we can best understand deep learning by implementing algorithms ourselves.\n",
    "\n",
    "First we import some libraries and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from PIL import Image\n",
    "import idx2numpy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the MNIST handwritten digit data set. The training set consists of 60,000 examples and the test set has 10,000 examples. Each example is a 28X28 greyscale image, formatted as a two-dimensional array with values ranging from 0 to 255, corresponding to black and white pixels respectively.\n",
    "\n",
    "It is necessary for every training and test example to be treated as a one-dimensional array, and so we reshape the examples into arrays with 28 * 28 = 784 elements.\n",
    "\n",
    "In the given data sets, the output value is an integer from 0 to 9, indicating the digit. We reshape the outputs using a One Hot Encoding to an array with all zeroes except for a one in the position of the value of the digit. For example, 3 is represented as [0,0,0,1,0,0,0,0,0,0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_X = idx2numpy.convert_from_file(\"train-images-idx3-ubyte\")\n",
    "train_y = idx2numpy.convert_from_file(\"train-labels-idx1-ubyte\")\n",
    "test_X  = idx2numpy.convert_from_file(\"t10k-images-idx3-ubyte\")\n",
    "test_y  = idx2numpy.convert_from_file(\"t10k-labels-idx1-ubyte\")\n",
    "# Reshape the inputs so there is a 1D array of features for every input.\n",
    "train_X = train_X.reshape(train_X.shape[0],train_X.shape[1]*train_X.shape[2])\n",
    "test_X  = test_X.reshape(test_X.shape[0],test_X.shape[1]*test_X.shape[2])\n",
    "# Encode the training and test y values in a manner that is easier to use in the cost function\n",
    "train_y_ohe = np.zeros((len(train_y),10))\n",
    "train_y_ohe[np.arange(len(train_y)), train_y] = 1\n",
    "test_y_ohe = np.zeros((len(test_y),10))\n",
    "test_y_ohe[np.arange(len(test_y)), test_y] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set the model hyperparameters. In this implementation, there are four hyperparameters to set: the learning rate, the number of hidden layers, the size of the hidden layers, and the number of iterations.\n",
    "\n",
    "It appears, through experimentation, that it is necessarily to lower the learning rate as the training of the model progresses. Consequently, the learning rate hyperparameter is treated as a function of the iteration number rather than a fixed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "def learning_rate(iter_num):\n",
    "    return min(0.01, 0.05/math.sqrt(iter_num+1))\n",
    "# Layer sizes. The array can be any length.\n",
    "# It must start with train_X.shape[1] (number of inputs) and end with 10 (number of digits)\n",
    "layer_sizes = [train_X.shape[1],100, 50, 10]\n",
    "num_iterations = 150 # Number of full (forward and back propagation) iterations of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the activation functions. All layers will use a rectified linear unit (ReLU) as the activation, except for the output layer, which uses a softmax function.\n",
    "\n",
    "For backpropagation, we need to calculate the derivative of the ReLU function as well, given as dReLU()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a is a vector\n",
    "def ReLU(a):\n",
    "    return np.maximum(a,0)\n",
    "def dReLU(a):\n",
    "    return (a>0).astype(int)\n",
    "def softmax(a):\n",
    "    return np.exp(a) / np.sum(np.exp(a),axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construction a randomized initial network. The weights W should not all start as zero, since in that case it will be impossible for backpropagation to break symmetry, and the network would essentially collapse into a single node for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_init_network(X):\n",
    "    W = [0]+[np.random.randn(layer_sizes[i],layer_sizes[i+1])*0.01 for i in range(len(layer_sizes)-1)]\n",
    "    b = [0]+[np.zeros((1,layer_sizes[i+1])) for i in range(len(layer_sizes)-1)]\n",
    "    W[0] = X\n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation. Given a network W,b and inputs X, determine the predicted and intermediate network values. The predictions are A[len(layer_sizes)-1] (the final entry of the array A). The intermediate values are returned because they are necessary in the calculation of derivatives in the backpropagation stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply forward propagation on a set of training examples\n",
    "def forward_prop(X,W,b):\n",
    "    A = [X]\n",
    "    Z = [0]\n",
    "    A_cur = X\n",
    "    for i in range(len(layer_sizes)-1):\n",
    "        Z_cur = np.add(np.matmul(A_cur,W[i+1]),b[i+1])\n",
    "        if i < len(layer_sizes)-2:\n",
    "            A_cur = ReLU(Z_cur)\n",
    "        else:\n",
    "            A_cur = softmax(Z_cur)\n",
    "        A.append(A_cur)\n",
    "        Z.append(Z_cur)\n",
    "    return A,Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the cost fuction. The cost is a measure of how far the predicted values y_pred are from the actual values y. A lower cost indicates a more accurate network. We use a cross-entropy cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cost function, using predicted and actual y values\n",
    "def cost(y_pred, y):\n",
    "    return -1./len(y) * np.sum(np.add( np.multiply(y, np.log(y_pred)), np.multiply(1-y,np.log(1-y_pred)) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the derivatives of difference between predicted and actual values, with respect to network parameters W and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate derivatives to be used in backprop\n",
    "def calc_derivatives(X,W,b,A,Z,y):\n",
    "    dZ = [0 for i in range(len(layer_sizes))]\n",
    "    dW = [0 for i in range(len(layer_sizes))]\n",
    "    db = [0 for i in range(len(layer_sizes))]\n",
    "\n",
    "    dZ[len(layer_sizes)-1] = A[len(layer_sizes)-1]-y\n",
    "    dW[len(layer_sizes)-1] = 1./len(X)*np.matmul(A[len(layer_sizes)-2].T,dZ[len(layer_sizes)-1])\n",
    "    db[len(layer_sizes)-1] = 1./len(X)*np.sum(dZ[len(layer_sizes)-1],axis=0,keepdims=True)\n",
    "\n",
    "    for layer in range(len(layer_sizes)-2,0,-1):\n",
    "        dZ[layer] = np.multiply( np.matmul(dZ[layer+1],W[layer+1].T), dReLU(Z[layer]) )\n",
    "        dW[layer] = 1./len(X) * np.matmul(A[layer-1].T,dZ[layer])\n",
    "        db[layer] = 1./len(X) * np.sum(dZ[layer],axis=0,keepdims=True)\n",
    "    return dZ, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation. Here we adjust the network parameters W and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(X,W,b,A,Z,y,iter_num):\n",
    "    dZ, dW, db = calc_derivatives(X,W,b,A,Z,y)\n",
    "    for i in range(1,len(layer_sizes)):\n",
    "        W[i] -= learning_rate(iter_num) * dW[i]\n",
    "        b[i] -= learning_rate(iter_num) * db[i]\n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def nn_iterate(X,W,b,y):\n",
    "#    A,Z = forward_prop(X,W,b)\n",
    "#    W,b = backprop(X,W,b,A,Z,y)\n",
    "#    return W,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and train a neural network. Each iteration performs a forward and backpropagation step. The result should be a trained network that reasonably accurately identifies digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(X,y):\n",
    "    W,b = random_init_network(X)\n",
    "    for i in range(num_iterations):\n",
    "        A,Z = forward_prop(X,W,b)\n",
    "        W,b = backprop(X,W,b,A,Z,y,i)\n",
    "        if (i+1)%5 == 0:\n",
    "            print \"Iteration \" + str(i+1) + \": \" + str(cost(A[len(layer_sizes)-1],y))\n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the network. This step might take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5: 3.22346182847\n",
      "Iteration 10: 3.15871947487\n",
      "Iteration 15: 2.98037059772\n",
      "Iteration 20: 2.54447874134\n",
      "Iteration 25: 2.0300854004\n",
      "Iteration 30: 5.34294014897\n",
      "Iteration 35: 2.46298843312\n",
      "Iteration 40: 2.26037498586\n",
      "Iteration 45: 2.06350126234\n",
      "Iteration 50: 1.80896355152\n",
      "Iteration 55: 1.50904240143\n",
      "Iteration 60: 1.31253710244\n",
      "Iteration 65: 1.70890548549\n",
      "Iteration 70: 1.18816506597\n",
      "Iteration 75: 1.05986324879\n",
      "Iteration 80: 0.982870058645\n",
      "Iteration 85: 0.927800305229\n",
      "Iteration 90: 0.885179649291\n",
      "Iteration 95: 0.850777075098\n",
      "Iteration 100: 0.822188582512\n",
      "Iteration 105: 0.797907527346\n",
      "Iteration 110: 0.776927269238\n",
      "Iteration 115: 0.758485900509\n",
      "Iteration 120: 0.742061600961\n",
      "Iteration 125: 0.727317480478\n",
      "Iteration 130: 0.713952496958\n",
      "Iteration 135: 0.701764270243\n",
      "Iteration 140: 0.690570302303\n",
      "Iteration 145: 0.680229561968\n",
      "Iteration 150: 0.670620383628\n"
     ]
    }
   ],
   "source": [
    "W,b = build_network(train_X,train_y_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how it performs. We will apply the network trained above on the test set and report the accuracy. The value will vary because of randomness in the initial weights, but it should be around 0.9, meaning that about 90% of examples in the test set are correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.8901\n"
     ]
    }
   ],
   "source": [
    "def test_network(X,W,b,y):\n",
    "    A,Z = forward_prop(X,W,b)\n",
    "    y_pred = A[-1]\n",
    "    y_pred = [np.where(y_pred[i]==max(y_pred[i]))[0][0] for i in range(len(y_pred))]\n",
    "    return y_pred\n",
    "y_pred = test_network(test_X,W,b,test_y_ohe)\n",
    "accuracy = len([i for i in range(len(y_pred)) if y_pred[i] == test_y[i]]) / float(len(y_pred))\n",
    "print \"Accuracy is \" + str(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "The network's performance of about 90% suggests that the algorithm is working, though with a considerably worse performance than the algorithms reported by LeCun et al.\n",
    "\n",
    "Little work was done to optimize the hyperparameters. If a more systematic hyperparameter search was undertaken, it would be appropriate to divide the training set into training and dev sets, to use the dev set to optimize hyperparameters, and to use the test set to evaluate network performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
